{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Kullback-Leibler estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import KDTree\n",
    "from typing import Literal\n",
    "from sklearn.preprocessing import KBinsDiscretizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical CDF estimator\n",
    "\n",
    "Proposed by [Fernando PÃ©rez-Cruz](https://ieeexplore.ieee.org/document/4595271)\n",
    "\n",
    "Implementation based on post by [kjetil b halvorsen](https://stats.stackexchange.com/questions/211175/kullback-leibler-divergence-for-two-samples/248657#248657)\n",
    "with Run Length Encoding code by [Thomas Browne](https://stackoverflow.com/a/32681075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle(inarray):\n",
    "    \"\"\"Run Length Encoding. Doesn't perform sort internally.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inarray : 1d array-like\n",
    "        Array to be encoded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    z : np.ndarray\n",
    "        Run lengths\n",
    "    p : np.ndarray\n",
    "        Positions\n",
    "    v : np.ndarray\n",
    "        Values\n",
    "    \"\"\"\n",
    "    ia = np.asarray(inarray)  # force numpy\n",
    "    n = len(ia)\n",
    "    if n == 0:\n",
    "        return (None, None, None)\n",
    "    else:\n",
    "        y = ia[1:] != ia[:-1]  # pairwise unequal (string safe)\n",
    "        i = np.append(np.where(y), n - 1)  # must include last element posi\n",
    "        z = np.diff(np.append(-1, i))  # run lengths\n",
    "        p = np.cumsum(np.append(0, z))[:-1]  # positions\n",
    "        return (z, p, ia[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(X, X_min=None, X_max=None):\n",
    "    \"\"\"Empirical CDF (cumulative distribution funciton) from random sample.\n",
    "\n",
    "    Continous approximation via linear interpolation.\\n\n",
    "    Returns function which can be used to get value of empirical CDF for any point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 1d array-like\n",
    "        Random sample\n",
    "    X_min : float, default=None\n",
    "        Left limit of the domain\n",
    "    X_max : 1d array-like, default=None\n",
    "        Right limit of the domain\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cdf : function\n",
    "        Empirical CDF\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, copy=True)\n",
    "    n = len(X)\n",
    "    X.sort()\n",
    "\n",
    "    X_u = np.unique(X)\n",
    "    # In case there are duplicates perform rle\n",
    "    if n != len(X_u):\n",
    "        X_rle, _, _ = rle(X)\n",
    "    else:\n",
    "        X_rle = np.ones_like(X)\n",
    "\n",
    "    # Points going through middle of step function centered at points from the sample (in case duplicates stacked steps)\n",
    "    y = (np.cumsum(X_rle) - 0.5 * X_rle) / n\n",
    "    # Attach left and right limit for proper interpolation\n",
    "    if (X_min is not None) and (X_u[0] != X_min):\n",
    "        X_u = np.concatenate([[X_min], X_u])\n",
    "        y = np.concatenate([[0], y])\n",
    "    if (X_max is not None) and (X_u[-1] != X_max):\n",
    "        X_u = np.concatenate([X_u, [X_max]])\n",
    "        y = np.concatenate([y, [1]])\n",
    "        \n",
    "    return functools.partial(np.interp, xp=X_u, fp=y, left=0, right=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div(X, Y, eps=None, num_instab: Literal[\"warning\", \"ignore\"] = \"warning\"):\n",
    "    \"\"\"Kullback-Leibler divergence (relative entropy) estimator.\n",
    "\n",
    "    According to Perez-Cruz algorithm using empirical CDFs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 1d array-like\n",
    "        Random sample (\"true\" distribution).\n",
    "    Y : 1d array-like\n",
    "        Random sample (\"model\" distribution).\n",
    "    eps : float, default=None\n",
    "        epsilon for calculation of CDFs differential.\n",
    "    num_instab : {'warning', 'ignore'}, default='warning'\n",
    "        How to handle numerical instability caused by close to horizontal CDF of Y.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kl_div : float\n",
    "        Estimation of Kullback-Leibler divergence.\n",
    "    \"\"\"\n",
    "    # Force numpy array\n",
    "    X = np.asarray(X, copy=True)\n",
    "    Y = np.asarray(Y, copy=True)\n",
    "\n",
    "    if eps is None:\n",
    "        # Calculate epsilon two times smaller than smallest spacing between points in each random sample\n",
    "        X_u = np.unique(X)\n",
    "        X_u.sort()\n",
    "        Y_u = np.unique(Y)\n",
    "        Y_u.sort()\n",
    "        dx = np.diff(X_u)\n",
    "        dy = np.diff(Y_u)\n",
    "        eps = np.concat([dx, dy]).min() / 2\n",
    "\n",
    "    tot_domain = np.concat([X, Y])\n",
    "    tot_min = tot_domain.min() - eps\n",
    "    tot_max = tot_domain.max()\n",
    "\n",
    "    # \"Stretch\" Q CDF to avoid division by zero\n",
    "    P = ecdf(X)\n",
    "    Q = ecdf(Y, X_min=tot_min, X_max=tot_max)\n",
    "\n",
    "    dP = P(X) - P(X - eps)\n",
    "    dQ = Q(X) - Q(X - eps)\n",
    "\n",
    "    mask_dP = dP != 0.0\n",
    "    mask_dQ = dQ != 0.0\n",
    "    mask = mask_dP & mask_dQ\n",
    "\n",
    "    # Raise warning\n",
    "    if num_instab == \"warning\":\n",
    "        if (problems := (~mask_dQ).sum()) != 0:\n",
    "            print(\n",
    "                f\"{problems} point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\"\n",
    "            )\n",
    "\n",
    "    # Non-singular elemnts\n",
    "    odds = dP[mask] / dQ[mask]\n",
    "    # Include zeros of dP - since they contribute 0, we need to only modify normalization n\n",
    "    n = len(odds) + (~mask_dP).sum()\n",
    "    result = np.log(odds).sum() / n - 1\n",
    "\n",
    "    # Force non-negative result\n",
    "    return max(result, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn estimator\n",
    "\n",
    "Proposed by [Wang et al.](https://ieeexplore.ieee.org/document/4839047)\n",
    "\n",
    "Code by [Nathan Hartland](https://github.com/nhartland/KL-divergence-estimators/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_sample_shapes(s1, s2, k):\n",
    "    # Expects [N, D]\n",
    "    assert len(s1.shape) == len(s2.shape) == 2\n",
    "    # Check dimensionality of sample is identical\n",
    "    assert s1.shape[1] == s2.shape[1]\n",
    "    \n",
    "def scipy_estimator(s1, s2, k=1):\n",
    "    \"\"\"KL-Divergence estimator using scipy's KDTree\n",
    "    s1: (N_1,D) Sample drawn from distribution P\n",
    "    s2: (N_2,D) Sample drawn from distribution Q\n",
    "    k: Number of neighbours considered (default 1)\n",
    "    return: estimated D(P|Q)\n",
    "    \"\"\"\n",
    "    verify_sample_shapes(s1, s2, k)\n",
    "\n",
    "    n, m = len(s1), len(s2)\n",
    "    d = float(s1.shape[1])\n",
    "    D = np.log(m / (n - 1))\n",
    "\n",
    "    nu_d, _ = KDTree(s2).query(s1, k)\n",
    "    rho_d, _ = KDTree(s1).query(s1, k + 1)\n",
    "\n",
    "    # KTree.query returns different shape in k==1 vs k > 1\n",
    "    if k > 1:\n",
    "        D += (d / n) * np.sum(np.log(nu_d[::, -1] / rho_d[::, -1]))\n",
    "    else:\n",
    "        D += (d / n) * np.sum(np.log(nu_d / rho_d[::, -1]))\n",
    "\n",
    "    return max(D, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div_bin(X, Y, n_bins=10, strategy=\"quantile\", verbose=False):\n",
    "    n_x = len(X)\n",
    "    n_y = len(Y)\n",
    "    binner = KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", strategy=strategy)\n",
    "    X_binned = binner.fit_transform(X)\n",
    "    Y_binned = binner.transform(Y)\n",
    "\n",
    "    result = 0\n",
    "    for i in range(n_bins):\n",
    "        y_count = (Y_binned == i).sum()\n",
    "\n",
    "        if y_count == 0:\n",
    "            if verbose:\n",
    "                print(\"Empty bin for Y - value omitted\")\n",
    "            continue\n",
    "\n",
    "        x_count = (X_binned == i).sum()\n",
    "        \n",
    "        result += x_count/n_x * np.log((x_count/n_x)/(y_count/n_y))\n",
    "    return max(0,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytic KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div_norm_norm(mu_1, sigma_1, mu_2, sigma_2):\n",
    "    return ((mu_1 - mu_2) ** 2 + sigma_1**2 - sigma_2**2) / (\n",
    "        2 * sigma_2**2\n",
    "    ) + np.log(sigma_2 / sigma_1)\n",
    "\n",
    "\n",
    "def kl_div_uni_uni(a_1, b_1, a_2, b_2):\n",
    "    return np.log((b_2 - a_2) / (b_1 - a_1))\n",
    "\n",
    "\n",
    "def kl_div_exp_exp(lambda_1, lambda_2):\n",
    "    return np.log(lambda_1 / lambda_2) + lambda_2 / lambda_1 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-divergence standard normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical CDF estimator - 0.0004567471939944667\n",
      "knn estimator           - 0\n",
      "Binning estimator       - 0.00011878705189443382\n",
      "Analytical result       - 0.0\n"
     ]
    }
   ],
   "source": [
    "mu_1 = 0\n",
    "sigma_1 = 1\n",
    "mu_2 = mu_1\n",
    "sigma_2 = sigma_1\n",
    "rng = np.random.default_rng(seed=42)\n",
    "X = rng.normal(mu_1, sigma_1, 100_000)\n",
    "Y = rng.normal(mu_2, sigma_2, 100_000)\n",
    "print(f\"Empirical CDF estimator - {kl_div(X, Y)}\")\n",
    "print(f\"knn estimator           - {scipy_estimator(X.reshape(-1,1), Y.reshape(-1,1))}\")\n",
    "print(f\"Binning estimator       - {kl_div_bin(X.reshape(-1,1), Y.reshape(-1,1))}\")\n",
    "print(f\"Analytical result       - {kl_div_norm_norm(mu_1, sigma_1, mu_2, sigma_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two normal distributions - $\\mathcal{N}(0,1)$ and $\\mathcal{N}(0,2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical CDF estimator - 0.3148033341857741\n",
      "knn estimator           - 0.31550281847300415\n",
      "Binning estimator       - 0.23010721551018237\n",
      "Analytical result       - 0.3181471805599453\n"
     ]
    }
   ],
   "source": [
    "mu_1 = 0\n",
    "sigma_1 = 1\n",
    "mu_2 = 0\n",
    "sigma_2 = 2\n",
    "rng = np.random.default_rng(seed=42)\n",
    "X = rng.normal(mu_1, sigma_1, 100_000)\n",
    "Y = rng.normal(mu_2, sigma_2, 100_000)\n",
    "print(f\"Empirical CDF estimator - {kl_div(X, Y)}\")\n",
    "print(f\"knn estimator           - {scipy_estimator(X.reshape(-1,1), Y.reshape(-1,1))}\")\n",
    "print(f\"Binning estimator       - {kl_div_bin(X.reshape(-1,1), Y.reshape(-1,1))}\")\n",
    "print(f\"Analytical result       - {kl_div_norm_norm(mu_1, sigma_1, mu_2, sigma_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two normal distributions - $\\mathcal{N}(0,1)$ and $\\mathcal{N}(0,0.2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6898 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "Empirical CDF estimator - 4.146234064438398\n",
      "knn estimator           - 4.207346717859741\n",
      "Binning estimator       - 1.97754059004881\n",
      "Analytical result       - 10.390562087565897\n"
     ]
    }
   ],
   "source": [
    "mu_1 = 0\n",
    "sigma_1 = 1\n",
    "mu_2 = 0\n",
    "sigma_2 = 0.2\n",
    "rng = np.random.default_rng(seed=42)\n",
    "X = rng.normal(mu_1, sigma_1, 100_000)\n",
    "Y = rng.normal(mu_2, sigma_2, 100_000)\n",
    "print(f\"Empirical CDF estimator - {kl_div(X, Y)}\")\n",
    "print(f\"knn estimator           - {scipy_estimator(X.reshape(-1,1), Y.reshape(-1,1))}\")\n",
    "print(f\"Binning estimator       - {kl_div_bin(X.reshape(-1,1), Y.reshape(-1,1))}\")\n",
    "print(f\"Analytical result       - {kl_div_norm_norm(mu_1, sigma_1, mu_2, sigma_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "956 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "1878 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "2080 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "1731 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "2336 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "2203 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "650 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "1419 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "378 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "1642 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "529 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "1089 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "812 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "855 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "497 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "1197 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "1724 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "1491 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "781 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "952 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "947 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "1710 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "639 point removed due to zeros encounter in dQ. This suggests numerical instability - return value might not be reliable. Try dividing sample in few parts and taking average result.\n",
      "999\n",
      "Empirical CDF estimator          - 3.951123498073733 +/- 0.4454386667139436\n",
      "Empirical CDF estimator averaged - 4.14642208406058 +/- 0.12968182969383932\n",
      "knn estimator                    - 4.396171911959766 +/- 0.07136277428268406\n",
      "Binning estimator                - 1.6355941062122235 +/- 0.5385350060505641\n",
      "Analytical result                - 10.390562087565897\n"
     ]
    }
   ],
   "source": [
    "mu_1 = 0\n",
    "sigma_1 = 1\n",
    "mu_2 = 0\n",
    "sigma_2 = 0.2\n",
    "size = 100_000\n",
    "rng = np.random.default_rng(seed=42)\n",
    "ecdf_kl = np.array([])\n",
    "ecdf_kl_avg = np.array([])\n",
    "knn_kl = np.array([])\n",
    "bin_kl = np.array([])\n",
    "\n",
    "n = 1000\n",
    "n_chunks = 10\n",
    "for i in range(n):\n",
    "    X = rng.normal(mu_1, sigma_1, size)\n",
    "    Y = rng.normal(mu_2, sigma_2, size)\n",
    "\n",
    "    ecdf_kl = np.append(ecdf_kl, kl_div(X, Y, num_instab=\"ignore\"))\n",
    "    for j in range(n_chunks):\n",
    "        ecdf_kl_avg = np.append(ecdf_kl_avg, kl_div(X[j*size//n_chunks:(j+1)*size//n_chunks],Y[j*size//n_chunks:(j+1)*size//n_chunks]))\n",
    "\n",
    "    knn_kl = np.append(knn_kl, scipy_estimator(X.reshape(-1,1), Y.reshape(-1,1)))\n",
    "    bin_kl = np.append(bin_kl, kl_div_bin(X.reshape(-1,1), Y.reshape(-1,1)))\n",
    "    print(i, flush=True, end=\"\\r\")\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Empirical CDF estimator          - {ecdf_kl.mean()} +/- {ecdf_kl.std()}\")\n",
    "print(f\"Empirical CDF estimator averaged - {ecdf_kl_avg.mean()} +/- {ecdf_kl_avg.std()}\")\n",
    "print(f\"knn estimator                    - {knn_kl.mean()} +/- {knn_kl.std()}\")\n",
    "print(f\"Binning estimator                - {bin_kl.mean()} +/- {bin_kl.std()}\")\n",
    "print(f\"Analytical result                - {kl_div_norm_norm(mu_1, sigma_1, mu_2, sigma_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.hist(X, bins=100, density=True, alpha=0.6)\n",
    "# plt.hist(Y, bins=100, density=True, alpha=0.6)\n",
    "plt.hist(np.append(X,Y), bins=100, density=True, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
