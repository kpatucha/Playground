{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual information for features with missing values\n",
    "\n",
    "*From my stackexchange question and answer [link](https://stats.stackexchange.com/questions/661624/encoding-high-proportion-of-missing-values-for-continuous-variable-results-in-mu/661625)\n",
    "\n",
    "Calculating mutual information using `sklearn.feature_selection.mutual_info_classif` for a feature with high proportion of missing values can give misleading results. Especially when there are many entries with the same value of a feature (like e.g. many missing values encoded as single number).\n",
    "\n",
    "Let's consider extreme example:\n",
    "- `X`: 10 000 numbers drawn from $Exponential(1)$ distribution and 80 000 missing values\n",
    "- `y`: binary variable with only 100 \"ones\" (only about 0.15%) which is completely independent of `X`\n",
    "\n",
    "Since `mutual_info_classif` doesn't accept missing values have to be encoded as a value outside range of non-missing part (e.g. `-999`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.special import xlogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of y from mutual_info_classif:                    H(y) = 0.008668710002102975\n",
      "Entropy of y calculated:                                  H(y) = 0.008668710002103328\n",
      "Mutual information of X and y from mutual_info_classif: I(X,y) = 0.016368695523635624\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "X = rng.exponential(1, 10_000)\n",
    "# Simulate encoding nulls\n",
    "X = np.append(X, -999 * np.ones(80_000))\n",
    "\n",
    "# Completely independent target\n",
    "y = np.zeros_like(X)\n",
    "y_1 = rng.choice(list(range(len(X))), 100, replace=False)\n",
    "y[y_1] = 1\n",
    "\n",
    "print(f\"Entropy of y from mutual_info_classif:                    H(y) = {mutual_info_classif(y[:,None], y, discrete_features=True)[0]}\")\n",
    "print(f\"Entropy of y calculated:                                  H(y) = {xlogy(y.mean(), 1/y.mean())+xlogy(1-y.mean(), 1/(1-y.mean()))}\")\n",
    "print(f\"Mutual information of X and y from mutual_info_classif: I(X,y) = {mutual_info_classif(X[:,None], y)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result does not fulfill the property of mutual information\n",
    "$$\n",
    "I(X; Y) \\leq \\min(H(X), H(Y))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is caused by how [KSG estimator](https://arxiv.org/abs/cond-mat/0305641) works (which is what is used in `mutual_info_classif`). It is based on idea of nearest-neighbours counting, which might give incorrect results in case when there are duplicates in the continuous feature. It is partially solved by addition of small amount of noise to the feature (which is why there is `random_state` parameter in `mutual_info_classif` which controls the seed for this noise) but in such extreme cases it fails.\n",
    "\n",
    "This problem can be solved by introduction of helper binary variable $X_{NA}$ that indicates whether the feature is null or not for a given observation. Feature can also be interpreted as *mixed random variable* where missingness is discrete part and the rest is continuous part. Then using chain-rule of mutual information we get this formula:\n",
    "$$\n",
    "\\begin{split}\n",
    "I(X; Y) &= I(X_{NA}; Y) + I(X;Y\\mid X_{NA})\\\\\n",
    "&= I(X_{NA}; Y) + P(X_{NA}=0) I(X\\mid X_{NA}=0;Y\\mid X_{NA}=0)\\\\\n",
    "&= I(X_{NA}; Y) + P(X_{NA}=0) I_c(X;Y)\n",
    "\\end{split}\n",
    "$$\n",
    "where\n",
    "- $X_{NA}=1$ when $X$ is missing and $X_{NA}=0$ when $X$ is not missing\n",
    "- $X\\mid X_{NA}=0$ and $Y\\mid X_{NA}=0$ is a subset of the data where $X$ is *non-missing*\n",
    "- $I_c(X;Y)$ is mutual information estimation for part of the data that has *non-mising* $X$\n",
    "\n",
    "So first we calculate mutual information of missingness and $Y$ and then we add to it mutual information of *non-missing part* scaled by probability of observing *non-missing part*. This form is quite useful since it allows using existing scikit-learn functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info_classif_mixed(x, y, special_code=None, **kwargs):\n",
    "    if not isinstance(x, pd.Series):\n",
    "        x = pd.Series(x)\n",
    "\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "\n",
    "    # Binary variable indicating \"missingness\" (either for NaNs or encoded with special_code)\n",
    "    if special_code is None:\n",
    "        x_d = (x.isna()).astype(int)\n",
    "    else:\n",
    "        x_d = (x == special_code).astype(int)\n",
    "\n",
    "    # Non-missing part of variables\n",
    "    x_c = x[x_d == 0]\n",
    "    y_c = y[x_d == 0]\n",
    "\n",
    "    # Discrete part of MI\n",
    "    mi_d = mutual_info_classif(\n",
    "        x_d.values[:, None], y, discrete_features=True, **kwargs\n",
    "    )[0]\n",
    "\n",
    "    # Continuous part of MI\n",
    "    mi_c = mutual_info_classif(\n",
    "        x_c.values[:, None], y_c, discrete_features=False, **kwargs\n",
    "    )[0]\n",
    "\n",
    "    return mi_d + (1 - x_d.mean()) * mi_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of y from mutual_info_classif:                    H(y) = 0.008668710002102977 +/- 1.734723475976807e-18\n",
      "Entropy of y calculated:                                  H(y) = 0.008668710002103328\n",
      "Mutual information of X and y from mutual_info_classif: I(X,y) = 0.01610285624719649 +/- 0.00040891585424112536\n",
      "Corrected mutual_info_classif:                          I(X,y) = 1.680313074239922e-05 +/- 2.0923604220299665e-05\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "n = 1000\n",
    "entropy_list = []\n",
    "mi_list = []\n",
    "mi_mixed_list = []\n",
    "for i in range(n):\n",
    "    print(i, flush=True, end=\"\\r\")\n",
    "    X = rng.exponential(1, 10_000)\n",
    "    X = np.append(X, -999 * np.ones(80_000))\n",
    "\n",
    "    y = np.zeros_like(X)\n",
    "    y_1 = rng.choice(list(range(len(X))), 100, replace=False)\n",
    "    y[y_1] = 1\n",
    "\n",
    "    entropy_list.append(mutual_info_classif(y[:,None],y, discrete_features=True)[0])\n",
    "    mi_list.append(mutual_info_classif(X[:,None],y)[0])\n",
    "    mi_mixed_list.append(mutual_info_classif_mixed(X, y, special_code=-999))\n",
    "\n",
    "entropy_array = np.array(entropy_list)\n",
    "mi_array = np.array(mi_list)\n",
    "mi_mixed_array = np.array(mi_mixed_list)\n",
    "\n",
    "print(f\"Entropy of y from mutual_info_classif:                    H(y) = {entropy_array.mean()} +/- {entropy_array.std()}\")\n",
    "print(f\"Entropy of y calculated:                                  H(y) = {xlogy(y.mean(), 1/y.mean())+xlogy(1-y.mean(), 1/(1-y.mean()))}\")\n",
    "print(f\"Mutual information of X and y from mutual_info_classif: I(X,y) = {mi_array.mean()} +/- {mi_array.std()}\")\n",
    "print(f\"Corrected mutual_info_classif:                          I(X,y) = {mi_mixed_array.mean()} +/- {mi_mixed_array.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
